{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!pip install torchsde einops diffusers transformers accelerate peft timm kornia aiohttp\n",
    "!apt install -qqy\n",
    "\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\n",
    "!git clone https://github.com/ltdrdata/ComfyUI-Manager /content/ComfyUI/custom_nodes/ComfyUI-Manager\n",
    "!git clone https://github.com/lrzjason/Comfyui-In-Context-Lora-Utils /content/ComfyUI/custom_nodes/Comfyui-In-Context-Lora-Utils\n",
    "!git clone https://github.com/cubiq/ComfyUI_essentials /content/ComfyUI/custom_nodes/ComfyUI_essentials\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev.sft -d /content/ComfyUI/models/unet -o flux1-dev.sft\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/ComfyUI/models/clip -o clip_l.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp16.safetensors -d /content/ComfyUI/models/clip -o t5xxl_fp16.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/ComfyUI/models/vae -o ae.sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/ComfyUI\n",
    "\n",
    "import os, shutil, json, requests, random, time\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from nodes import load_custom_node\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "from comfy_extras import nodes_model_advanced, nodes_custom_sampler, nodes_flux\n",
    "\n",
    "load_custom_node(\"/content/ComfyUI/custom_nodes/Comfyui-In-Context-Lora-Utils\")\n",
    "load_custom_node(\"/content/ComfyUI/custom_nodes/ComfyUI_essentials\")\n",
    "\n",
    "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
    "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
    "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "LoraLoader = NODE_CLASS_MAPPINGS[\"LoraLoader\"]()\n",
    "LoadImage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
    "AddMaskForICLora = NODE_CLASS_MAPPINGS[\"AddMaskForICLora\"]()\n",
    "ModelSamplingFlux = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingFlux\"]()\n",
    "CLIPTextEncodeFlux = nodes_flux.NODE_CLASS_MAPPINGS[\"CLIPTextEncodeFlux\"]()\n",
    "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
    "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "SetLatentNoiseMask = NODE_CLASS_MAPPINGS[\"SetLatentNoiseMask\"]()\n",
    "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "VAEEncode  = NODE_CLASS_MAPPINGS[\"VAEEncode\"]()\n",
    "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "ImageCrop = NODE_CLASS_MAPPINGS[\"ImageCrop+\"]()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    unet = UNETLoader.load_unet(\"flux1-dev-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
    "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp16.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
    "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
    "\n",
    "def download_file(url, save_dir, file_name):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_suffix = os.path.splitext(urlsplit(url).path)[1]\n",
    "    file_name_with_suffix = file_name + file_suffix\n",
    "    file_path = os.path.join(save_dir, file_name_with_suffix)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    return file_path\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(input):\n",
    "    values = input[\"input\"]\n",
    "\n",
    "    input_image = values['input_image_check']\n",
    "    input_image = download_file(url=input_image, save_dir='/content/ComfyUI/input', file_name='input_image')\n",
    "    positive_prompt = values['positive_prompt']\n",
    "    seed = values['seed']\n",
    "    steps = values['steps']\n",
    "    guidance = values['guidance']\n",
    "    sampler_name = values['sampler_name']\n",
    "    scheduler = values['scheduler']\n",
    "    max_shift = values['max_shift']\n",
    "    base_shift = values['base_shift']\n",
    "    patch_mode = values['patch_mode']\n",
    "    output_length = values['output_length']\n",
    "    patch_color = values['patch_color']\n",
    "    custom_lora1_url = values['custom_lora1_url']\n",
    "    custom_lora1_file = download_file(url=custom_lora1_url, save_dir='/content/ComfyUI/models/loras', file_name='custom_lora1_file')\n",
    "    custom_lora1_file = os.path.basename(custom_lora1_file)\n",
    "    custom_lora1_strength_model = values['custom_lora1_strength_model']\n",
    "    custom_lora1_strength_clip = values['custom_lora1_strength_clip']\n",
    "    custom_lora2_url = values['custom_lora2_url']\n",
    "    custom_lora2_file = download_file(url=custom_lora2_url, save_dir='/content/ComfyUI/models/loras', file_name='custom_lora2_file')\n",
    "    custom_lora2_file = os.path.basename(custom_lora2_file)\n",
    "    custom_lora2_strength_model = values['custom_lora2_strength_model']\n",
    "    custom_lora2_strength_clip = values['custom_lora2_strength_clip']\n",
    "\n",
    "    if seed == 0:\n",
    "        random.seed(int(time.time()))\n",
    "        seed = random.randint(0, 18446744073709551615)\n",
    "    print(seed)\n",
    "\n",
    "    custom_lora1_unet, custom_lora1_clip = LoraLoader.load_lora(unet, clip, custom_lora1_file, custom_lora1_strength_model, custom_lora1_strength_clip)\n",
    "    lora_unet, lora_clip = LoraLoader.load_lora(custom_lora1_unet, custom_lora1_clip, custom_lora2_file, custom_lora2_strength_model, custom_lora2_strength_clip)\n",
    "    conditioning = CLIPTextEncodeFlux.encode(lora_clip, positive_prompt, positive_prompt, guidance)[0]\n",
    "    input_image = LoadImage.load_image(input_image)[0]\n",
    "    return_images, return_masks, min_x, min_y, target_width, target_height, width, height = AddMaskForICLora.add_mask(input_image, patch_mode, output_length, patch_color)\n",
    "    final_model = ModelSamplingFlux.patch(lora_unet, max_shift, base_shift, width, height)[0]\n",
    "    noise = RandomNoise.get_noise(seed)[0]\n",
    "    guider = BasicGuider.get_guider(final_model, conditioning)[0]\n",
    "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
    "    sigmas = BasicScheduler.get_sigmas(lora_unet, scheduler, steps, 1.0)[0]\n",
    "    latent_image = VAEEncode.encode(vae, return_images)[0]\n",
    "    latent_image = SetLatentNoiseMask.set_mask(latent_image, return_masks)[0]\n",
    "    sample, _ = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
    "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
    "    decoded = ImageCrop.execute(decoded, target_width, target_height, \"top-left\", min_x, min_y)[0]\n",
    "    image = Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(f\"/content/in-context-lora-{seed}-tost.png\")\n",
    "\n",
    "    result = f\"/content/in-context-lora-{seed}-tost.png\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = { \n",
    "    \"input\": {\n",
    "        \"input_image_check\": \"https://files.catbox.moe/z6mdko.png\",\n",
    "        \"positive_prompt\": \"This pair of images shows a pattern as [IMAGE1] Em1ru holding a packaging bag [IMAGE2]; the pattern on the packaging bag in [IMAGE2] must refer to [IMAGE1]; [IMAGE1] Em1ru holding a packaging bag; the packaging bag in [IMAGE2] is printed with [IMAGE1]; [IMAGE2] Em1ru holds a packaging bag with the [IMAGE1] pattern printed on it.\",\n",
    "        \"seed\": 0,\n",
    "        \"steps\": 40,\n",
    "        \"guidance\": 3.5,\n",
    "        \"sampler_name\": \"euler\",\n",
    "        \"scheduler\": \"simple\",\n",
    "        \"max_shift\": 1.15,\n",
    "        \"base_shift\": 0.50,\n",
    "        \"patch_mode\": \"auto\",\n",
    "        \"output_length\": 1536,\n",
    "        \"patch_color\": \"#FF0000\",\n",
    "        \"custom_lora1_url\": \"https://files.catbox.moe/4zgu8u.safetensors\",\n",
    "        \"custom_lora1_strength_model\": 1.0,\n",
    "        \"custom_lora1_strength_clip\": 1.0,\n",
    "        \"custom_lora2_url\": \"https://files.catbox.moe/e2hsa6.safetensors\",\n",
    "        \"custom_lora2_strength_model\": 0.9,\n",
    "        \"custom_lora2_strength_clip\": 0.9\n",
    "    }\n",
    "}\n",
    "image = generate(input)\n",
    "Image.open(image)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
